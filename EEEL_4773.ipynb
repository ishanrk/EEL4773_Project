{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c343dcb",
   "metadata": {},
   "source": [
    "# Dependencies Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60261ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.optim.lr_scheduler import StepLR    # learning rate scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bebccf",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285c2c2",
   "metadata": {},
   "source": [
    "The class below represents our final model and an instance of it is initialized with our tuned hyperparameters\n",
    "\n",
    "The convolutional layers of the model can be changed by giving an argument of a list with kernel size, number of filters, and dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, \n",
    "                 #conv_filters=[32, 64, 128],\n",
    "                 #kernel_sizes=[3,3,3], \n",
    "                 #dropout_rate=0.5\n",
    "                 conv_layers = [{\"out_ch\":32, \"kernel_size\":5, \"dropout_rate\":0.5},\n",
    "                                {\"out_ch\":64, \"kernel_size\":5, \"dropout_rate\":0.5},\n",
    "                                {\"out_ch\":128, \"kernel_size\":5, \"dropout_rate\":0.5}],\n",
    "                mlp_layers = {\"out_ch\":256, \"dropout_rate\":0.5}\n",
    "                 ):\n",
    "        super(ImageCNN, self).__init__()\n",
    "        layers = []\n",
    "        in_ch = 1  # single-channel input\n",
    "        for spec in conv_layers:\n",
    "            layers += [\n",
    "                nn.Conv2d(in_ch, spec[\"out_ch\"], kernel_size=spec[\"kernel_size\"], padding='same'),\n",
    "                nn.BatchNorm2d(spec[\"out_ch\"]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout2d(spec[\"dropout_rate\"]),\n",
    "            ]\n",
    "            in_ch = spec[\"out_ch\"]  # update input channels for next layer\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample = torch.zeros(1,1,100,100)\n",
    "            feat = self.conv(sample)\n",
    "        flat_dim = feat.view(1, -1).size(1)\n",
    "        self.fc1 = nn.Linear(flat_dim, mlp_layers[\"out_ch\"])\n",
    "        self.bn1 = nn.BatchNorm1d(mlp_layers[\"out_ch\"])\n",
    "        self.dropout = nn.Dropout(mlp_layers[\"dropout_rate\"])\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)                 # (batch, C, H, W)\n",
    "        x = x.view(x.size(0), -1)        # flatten\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)              # (batch, 256)\n",
    "        # x = self.mlp(x)                 # (batch, num_classes)\n",
    "        x = self.fc2(x)                 # (batch, num_classes)\n",
    "        return x\n",
    "\n",
    "    # Kaiming weight init\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# Model Instance and tuned hyperparameters\n",
    "conv_layers = [\n",
    "                {\"out_ch\":16, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":32, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":64, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":128, \"kernel_size\": 7, \"dropout_rate\": 0.3}\n",
    "                \n",
    "            ]\n",
    "model = ImageCNN(conv_layers=conv_layers, mlp_layers={\"out_ch\":256, \"dropout_rate\":0.5}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1cf4ae",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d80a0d",
   "metadata": {},
   "source": [
    "The cell below contains the main train function that can be run with different batch sizes, learning rates and epochs.\n",
    "\n",
    "It needs a matrix (np array) x_train with the feature data and a label vector y_train (np array) to train correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a580a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentations\n",
    "def get_augmented_transforms():\n",
    "     return transforms.Compose([\n",
    "         transforms.ToPILImage(),             # expects CxHxW or HxW\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(degrees=15),\n",
    "         transforms.RandomResizedCrop(size=100, scale=(0.8, 1.0)),\n",
    "         transforms.ToTensor(),               # gets 1x100x100 for 1 channel\n",
    "     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, x_train, y_train , epochs=25, lr=1e-3, batch_size = 64, device=None):\n",
    "\n",
    "    X = x_train.reshape(-1, 100, 100)  # reshape to (N, 100, 100)\n",
    "    X = X / 255.0  # normalize if needed\n",
    "    X = X[:, np.newaxis, :, :]  # add channel dim: (N, 1, 100, 100)\n",
    "    #  to tensors\n",
    "    X_train_tensor = torch.from_numpy(X)\n",
    "    y_train_tensor = torch.from_numpy(y_train)\n",
    "\n",
    "    augment = get_augmented_transforms()\n",
    "    augmented_imgs = torch.stack([augment(img) for img in X_train_tensor])\n",
    "\n",
    "\n",
    "    X_train_tensor = torch.cat([X_train_tensor, augmented_imgs], dim=0)\n",
    "\n",
    "    y_train_tensor = torch.cat([y_train_tensor, y_train_tensor], dim=0)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # create loader\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # lr ← lr * 0.1 every 10 epochs\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        avg_train_loss = train_loss / total\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        # for keeping track\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        scheduler.step()  # update learning rate\n",
    "        print(f\"Epoch {epoch}/{epochs} — \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Training Accuracy: {train_acc:.2f}% | \"\n",
    "              f\" — Current Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0db5ab",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d625b9",
   "metadata": {},
   "source": [
    "The test function below requires a trained model, a matrix (np array) x_test and a label vector y_test (np array)\n",
    "\n",
    "It outputs an array predictions and a test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaba1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test):\n",
    "    '''\n",
    "    Takes in two numpy arrays for features: x_test\n",
    "    and for labels: y_test\n",
    "    and outputs a vector of predictions,\n",
    "    and accuracy\n",
    "    '''\n",
    "    X = x_test\n",
    "    X = X.reshape(-1, 100, 100)\n",
    " \n",
    "    X /= 255.0\n",
    "\n",
    "    X = X[:, np.newaxis, :, :]  # add channel dim: (N, 1, 100, 100)\n",
    "    #  to tensors\n",
    "    X_tensor = torch.from_numpy(X)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds = model(X_tensor.to(device=device))\n",
    "\n",
    "    preds =  preds.argmax(dim=1)\n",
    "\n",
    "    accuracy = np.sum(preds.cpu().numpy()==y_test)/len(y_test)\n",
    "    print(\"Accuracy is: \", accuracy)\n",
    "    return preds, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad5d00",
   "metadata": {},
   "source": [
    "# Sample Test\n",
    "\n",
    "This is a sample test, to show how we use train and test. Ideally you would have two different x_train and x_test, but we use the same one here for demonstarting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [\n",
    "                {\"out_ch\":16, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":32, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":64, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":128, \"kernel_size\": 7, \"dropout_rate\": 0.3}\n",
    "                \n",
    "            ]\n",
    "model = ImageCNN(conv_layers=conv_layers, mlp_layers={\"out_ch\":256, \"dropout_rate\":0.5})\n",
    "\n",
    "feats_csv = 'x_train_project.csv'   \n",
    "labels_csv = 't_train_project.csv'\n",
    "\n",
    "x_train = pd.read_csv(feats_csv, header=None).values.astype(np.float32)\n",
    "y_train = pd.read_csv(labels_csv, header=None).values.squeeze().astype(np.int64)\n",
    "\n",
    "# train\n",
    "history = train(model, x_train, y_train, epochs=25, lr=1e-2)\n",
    "# expect high accuracy as testing on same data\n",
    "predictions, accuracy = test(model, x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bae2c1",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2afa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_hyperparameters(features_csv, labels_csv, batch_size=64, val_frac=0.2, random_state=42):\n",
    "    '''\n",
    "    Use this function while hyperparameter tuning\n",
    "    It returns a validation loader and a train loader\n",
    "    mode = \"final\" indicates it will just return a train loader\n",
    "    mode = \"hyperparameter\" indicates it will return a validation and train loader\n",
    "    mode = \"test\" indicates it will return a test loader and a train loader\n",
    "    '''\n",
    "    X = pd.read_csv(features_csv, header=None).values.astype(np.float32)\n",
    "    y = pd.read_csv(labels_csv, header=None).values.squeeze().astype(np.int64)\n",
    "\n",
    "    # reshape to (N, 100, 100), to make sure the images correctly formated\n",
    "    X = X.reshape(-1, 100, 100)\n",
    "    # normalize \n",
    "    # IMP: CHECK WHETHER THE IMAGE IS ALREADY NORMALIZED    \n",
    "    X /= 255.0\n",
    "\n",
    "    # convert to tensor and add channel dimension: (N, 1, 100, 100)\n",
    "    X_tensor = torch.from_numpy(X).unsqueeze(1)\n",
    "    y_tensor = torch.from_numpy(y)\n",
    "\n",
    "    # create dataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # get quick split to get validation and train set\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_frac, random_state=random_state)\n",
    "    train_idx, val_idx = next(sss.split(X, y))\n",
    "\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    val_ds   = Subset(dataset, val_idx)\n",
    "\n",
    "    # dataLoaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def train_hyperparameter(model, x_train_csv, y_train_csv , epochs=25, lr=1e-3, device=None):\n",
    "    train_loader, validation_loader = load_data_hyperparameters(x_train_csv, y_train_csv,batch_size=64)\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # lr ← lr * 0.5 every 5 epochs\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        avg_train_loss = train_loss / total\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        # valid\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in validation_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += y_batch.size(0)\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        val_acc = val_correct / val_total * 100\n",
    "        \n",
    "        # plotting\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        scheduler.step()  # update learning rate\n",
    "        print(f\"Epoch {epoch}/{epochs} — \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "              #f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\"\n",
    "              f\" — LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "param_grid = {\n",
    "        \"n_conv\":       [2, 3, 4],\n",
    "        \"dropout\":      [0.3, 0.5, 0.7],\n",
    "        \"kernel\":       [3, 5, 7],\n",
    "        \"out_ch\":       [32, 64 ,128],\n",
    "    }\n",
    "\n",
    "out_ch_progressions = {\n",
    "    2: [(16, 32), (32, 64), (64, 128)],\n",
    "    3: [(16, 32, 64), (32, 64, 128)],\n",
    "    4: [(16,32,64,128)]\n",
    "    # Add more if you support n_conv = 4, etc.\n",
    "    }\n",
    "results = []\n",
    "counter = 0\n",
    "for n_conv in param_grid[\"n_conv\"]:\n",
    "        for dropout, kernel, out_chs in product(param_grid[\"dropout\"], param_grid[\"kernel\"], out_ch_progressions[n_conv]):\n",
    "            conv_layers = [\n",
    "                {\"out_ch\": out_chs[i], \"kernel_size\": kernel, \"dropout_rate\": dropout}\n",
    "                for i in range(n_conv)\n",
    "            ]\n",
    "\n",
    "            print(f\"Training with config: {conv_layers}\")\n",
    "            # build model\n",
    "            model = ImageCNN(conv_layers=conv_layers, mlp_layers={\"out_ch\":256, \"dropout_rate\":0.5})\n",
    "            # train model\n",
    "            hist = train_hyperparameter(model, feats_csv, labels_csv, epochs=25, lr=1e-2)\n",
    "            # save results\n",
    "            results.append({\n",
    "                'n_conv': n_conv,\n",
    "                'dropout':dropout,\n",
    "                'kernel': kernel,\n",
    "                'out_ch': out_chs,\n",
    "                'train_loss': hist['train_loss'],\n",
    "                'val_loss':   hist['val_loss'],\n",
    "                'train_acc':  hist['train_acc'],\n",
    "                'val_acc':    hist['val_acc'],\n",
    "            })\n",
    "            print(hist['val_acc'])\n",
    "            counter += 1\n",
    "print(f\"Total combinations: {counter}\")\n",
    "\n",
    "    # convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "    # sort by final validation accuracy\n",
    "df['final_val_acc'] = df['val_acc'].apply(lambda x: x[-1])\n",
    "df_sorted = df.sort_values('final_val_acc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Top 5 configs by validation accuracy:\")\n",
    "print(df_sorted.head(5)[['n_conv','dropout','kernel','out_ch','final_val_acc']])\n",
    "\n",
    "print(\"Worst 5 configs by validation accuracy:\")\n",
    "print(df_sorted.tail(5)[['n_conv','dropout','kernel','out_ch','final_val_acc']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0407d",
   "metadata": {},
   "source": [
    "# Testing against other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other Tuned Models\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test):\n",
    "    preds = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds, average='weighted')\n",
    "    precision = precision_score(y_test, preds, average='weighted')\n",
    "    return acc, recall, precision\n",
    "\n",
    "def train_svm_classifier(X_train,y_train, X_test, y_test, pca_components=100):\n",
    "\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    X_train_reduced = pca.fit_transform(X_train)\n",
    "    X_test_reduced  = pca.transform(X_test)\n",
    "\n",
    "\n",
    "    clf = SVC(kernel='linear', C=1.0, gamma='scale')\n",
    "    clf.fit(X_train_reduced, y_train)\n",
    "\n",
    "    return evaluate_model(clf, X_test_reduced, y_test)\n",
    "\n",
    "def train_knn_with_pca(X_train, y_train, X_test, y_test, pca_components=100):\n",
    "\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    X_train_reduced = pca.fit_transform(X_train)\n",
    "    X_test_reduced  = pca.transform(X_test)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train_reduced, y_train)\n",
    "    \n",
    "    return evaluate_model(knn, X_test_reduced, y_test)\n",
    "\n",
    "def train_large_mlp_classifier(X_train,y_train, X_test, y_test, hidden_layers=(1024,512, 256, 128, 64)):\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layers, activation='relu',\n",
    "                        solver='adam', max_iter=300, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return evaluate_model(clf, X_test, y_test)\n",
    "\n",
    "# DATA LOADING FUNCTIONS\n",
    "\n",
    "def get_augmented_transforms():\n",
    "     return transforms.Compose([\n",
    "         transforms.ToPILImage(),             # expects CxHxW or HxW\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(degrees=15),\n",
    "         transforms.RandomResizedCrop(size=100, scale=(0.8, 1.0)),\n",
    "         transforms.ToTensor(),               # gets 1x100x100 for 1 channel\n",
    "     ])\n",
    "\n",
    "\n",
    "def load_data_experiment(features_csv, labels_csv, batch_size=64, test_frac=0.2, random_state=42):\n",
    "    '''\n",
    "    Use this function for a held our test set for final accuracy\n",
    "    It returns a test loader and a train loader\n",
    "    '''\n",
    "    X = pd.read_csv(features_csv, header=None).values.astype(np.float32)\n",
    "    y = pd.read_csv(labels_csv, header=None).values.squeeze().astype(np.int64)\n",
    "\n",
    "    X = X.reshape(-1, 100, 100)  # reshape to (N, 100, 100)\n",
    "    X = X / 255.0  # normalize if needed\n",
    "    X = X[:, np.newaxis, :, :]  # add channel dim: (N, 1, 100, 100)\n",
    "\n",
    "    # Initial split: train+val vs test\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_frac, random_state=random_state)\n",
    "    train_idx, test_idx = next(sss1.split(X, y))\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train)\n",
    "    y_train_tensor = torch.from_numpy(y_train)\n",
    "\n",
    "    augment = get_augmented_transforms()\n",
    "    augmented_imgs = torch.stack([augment(img) for img in X_train_tensor])\n",
    "\n",
    "\n",
    "    X_train_tensor = torch.cat([X_train_tensor, augmented_imgs], dim=0)\n",
    "\n",
    "    y_train_tensor = torch.cat([y_train_tensor, y_train_tensor], dim=0)\n",
    "\n",
    "    X_test_tensor = torch.from_numpy(X_test)\n",
    "    y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "    # Create datasets\n",
    "    train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train_experiment(model, x_train_csv, y_train_csv , epochs=25, lr=1e-3, device=None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    train_loader, test_loader = load_data_experiment(x_train_csv, y_train_csv,batch_size=64)\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # lr ← lr * 0.1 every 10 epochs\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        avg_train_loss = train_loss / total\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        scheduler.step()  # update learning rate\n",
    "        print(f\"Epoch {epoch}/{epochs} — \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "              #f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\"\n",
    "              f\" — LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    X_train, y_train = [], []\n",
    "    X_test, y_test = [], []\n",
    "    for data, labels in train_loader:\n",
    "        # Flatten each image to a 1D array\n",
    "        X_train.append(data.view(data.size(0), -1).numpy())\n",
    "        y_train.append(labels.numpy())\n",
    "    for data, labels in test_loader:\n",
    "        # Flatten each image to a 1D array\n",
    "        X_test.append(data.view(data.size(0), -1).numpy())\n",
    "        y_test.append(labels.numpy())\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    X_test = np.concatenate(X_test, axis=0)\n",
    "    y_test = np.concatenate(y_test, axis=0)\n",
    "    return history, X_train, y_train, X_test, y_test, test_loader\n",
    "\n",
    "\n",
    "conv_layers = [\n",
    "                {\"out_ch\":16, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":32, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":64, \"kernel_size\": 7, \"dropout_rate\": 0.3},\n",
    "        {\"out_ch\":128, \"kernel_size\": 7, \"dropout_rate\": 0.3}\n",
    "                \n",
    "            ]\n",
    "model = ImageCNN(conv_layers=conv_layers, mlp_layers={\"out_ch\":256, \"dropout_rate\":0.5}) \n",
    "history,X_train,y_train,X_test,y_test,test_loader = train_experiment(model, feats_csv, labels_csv, epochs=25, lr=1e-2)\n",
    "model.eval()\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "for X_batch, y_batch in test_loader:\n",
    "        X_all.append(X_batch)\n",
    "        y_all.append(y_batch)\n",
    "\n",
    "X_all = torch.cat(X_all)\n",
    "y_all = torch.cat(y_all)\n",
    "\n",
    "    # Step 2: Move to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X_all = X_all.to(device)\n",
    "y_all = y_all.to(device)\n",
    "\n",
    "    # Step 3: Forward pass in one go\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        outputs = model(X_all)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "    # Step 4: Move predictions and labels to CPU for evaluation\n",
    "y_true = y_all.cpu().numpy()\n",
    "y_pred = preds.cpu().numpy()\n",
    "\n",
    "    # Step 5: Metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Optional: class names (if you have them)\n",
    "    # Example: class_names = ['cat', 'dog', 'frog']\n",
    "num_classes = cm.shape[0]\n",
    "class_names = [str(i) for i in range(num_classes)]\n",
    "\n",
    "    # Create the display\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "       # Customize and plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(include_values=True, cmap='Blues', ax=ax, xticks_rotation='horizontal')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "    # plt.show()\n",
    "plt.savefig('ConfusionMatrix.eps', format='eps')\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "train_loss = history['train_loss']\n",
    "train_acc  = history['train_acc']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # Plot\n",
    "    # plt.figure(figsize=(10, 4))\n",
    "plt.figure()\n",
    "    # Training Loss\n",
    "    # plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss', color='tab:red')\n",
    "plt.plot(epochs, [x / 100 for x in train_acc], label='Train Accuracy', color='tab:blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss and Accuracy')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('TrainingLossAcc.eps', format='eps')\n",
    "\n",
    "\n",
    "    # # Training Accuracy\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(epochs, train_acc, label='Train Accuracy', color='tab:blue')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Accuracy')\n",
    "    # plt.title('Training Accuracy')\n",
    "    # plt.grid(True)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "hidden_layers_list = [\n",
    "    (1024, 512, 256, 128, 64),\n",
    "    (512, 256, 128, 64),\n",
    "    (256, 128, 64),\n",
    "    (1024, 1024, 512, 256, 128, 64),\n",
    "    (512, 512, 256, 128, 64),\n",
    "    (256, 256, 128, 64),\n",
    "    (128, 128, 64),\n",
    "    (2048, 1024, 512, 256, 128, 64),\n",
    "    (2048, 2048, 1024, 512, 256, 128, 64),\n",
    "    ]\n",
    "for hidden_layers in hidden_layers_list:\n",
    "        acc, f1, prec = train_large_mlp_classifier(X_train, y_train, X_test, y_test, hidden_layers=hidden_layers)\n",
    "        print(f\"RF → Accuracy: {acc:.2f}, F1: {f1:.2f}, Precision: {prec:.2f}, Hidden Layers: {hidden_layers}\")\n",
    "\n",
    "    # train those first \n",
    "    \n",
    "pca_components = [5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500]\n",
    "for n_component in pca_components:\n",
    "        acc, f1, prec = train_svm_classifier(X_train, y_train, X_test, y_test, pca_components=n_component)\n",
    "        print(f\"RF → Accuracy: {acc:.2f}, F1: {f1:.2f}, Precision: {prec:.2f}, PCA: {n_component}\")\n",
    "\n",
    "        acc, f1, prec = train_knn_with_pca(X_train, y_train, X_test, y_test, pca_components=n_component)\n",
    "        print(f\"RF → Accuracy: {acc:.2f}, F1: {f1:.2f}, Precision: {prec:.2f}, PCA: {n_component}\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
